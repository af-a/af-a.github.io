<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Task-oriented Visual Pose Estimation | Ahmed Abdelrahman </title> <meta name="author" content="Ahmed Abdelrahman"> <meta name="description" content="Vision-based, ML-driven 6D object pose estimation for manipulation tasks"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/sf_logo.png?b273003b6a145077506ffe0fd3f4dd9d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://af-a.github.io/projects/task_pose_estimation.html"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ahmed</span> Abdelrahman </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Profile </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Task-oriented Visual Pose Estimation</h1> <p class="post-description">Vision-based, ML-driven 6D object pose estimation for manipulation tasks</p> </header> <article> <p>As a contribution to the Horizon Europe project: <a href="https://www.eurobin-project.eu/" rel="external nofollow noopener" target="_blank">euROBIN</a>, I developed a reusable, modular object pose estimation software component, which is publicly available in the <a href="https://github.com/eurobin-wp1/tum-tb-perception" rel="external nofollow noopener" target="_blank">tum-tb-perception</a> Github repository. This work was presented in <a class="citation" href="#abdelrahman2025poseestimation">(Abdelrahman et al., 2025)</a> at the European Robotics Forum (2025) and was nominated for a best paper award. The software component was also used in a Robocup-style cooperative euROBIN competition by our team (TUM-MIRMI) as well as other competitors, particularly for a set of robot manipulation challenges on a “<a href="https://ieeexplore.ieee.org/document/10378967" rel="external nofollow noopener" target="_blank">Taskboard</a>” benchmark platform.</p> <div class="row"> <div class="col-md-8 offset-md-2"> <figure> <picture> <img src="/assets/img/projects/tum_tb_perception/tum_tb_perception_rviz_demo.gif" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A demonstration of the developed 6D pose estimation's results on an euROBIN taskboard platform, visualized here on RViz. Left: CNN-based object detection on RGB images. Right: Depth point cloud, annotated with estimated 3D positions of detected objects (spheres) and estimated orientations of some objects (coordinate frames). </div> <hr> <h2 id="implementation">Implementation</h2> <p>The pose estimator relies on data from an RGB-D sensor, such as an Intel Realsense camera, to determine the 6D pose of known objects and involves three stages:</p> <ol> <li>Object Recognition</li> <li>Position Estimation</li> <li>Orientation Estimation</li> </ol> <div class="row justify-content-center"> <div class="col-11"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/tum_tb_perception/tum_tb_perception_pose_estimation_pipeline_illustration-480.webp 480w,/assets/img/projects/tum_tb_perception/tum_tb_perception_pose_estimation_pipeline_illustration-800.webp 800w,/assets/img/projects/tum_tb_perception/tum_tb_perception_pose_estimation_pipeline_illustration-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/tum_tb_perception/tum_tb_perception_pose_estimation_pipeline_illustration.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Pipeline overview" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An overview of the pipeline. </div> <p>For initial <strong>object recognition</strong>, a conventional CNN model is trained to detect known objects from RGB images. In this case, we used a pre-trained Faster R-CNN model which was fine-tuned using <em>transfer learning</em>: a relatively small set of images containing the objects of interest in varying conditions comprised the training dataset.</p> <p>For <strong>position estimation</strong>, the resulting image detections and aligned depth images (point clouds) are then used to localize the detected objects in 3D space. We project all 3D depth points onto the 2D RGB image plane using <em>central projection</em>. Then, for each detected object’s bounding box, we find all 2D projected points that fall within its box; the corresponding unprojected 3D depth points then form the segment of the original point cloud that falls on the respective object. By taking the mean of each object’s point cloud segment, we obtain an estimate of its 3D position.</p> <p>For <strong>orientation estimation</strong>, we currently implement object-specific, heuristic point cloud processing operations. In the case of the taskboard platform that we considered (and depicted in the figures above), we utilize a priori knowledge of the board’s geometry and the estimated component positions to determine its orientation. In future work, we will target a more object-agnostic orientation estimation method.</p> <h3 id="software-framework">Software Framework</h3> <div class="row"> <div class="col-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/tum_tb_perception/ros_component_diagram-480.webp 480w,/assets/img/projects/tum_tb_perception/ros_component_diagram-800.webp 800w,/assets/img/projects/tum_tb_perception/ros_component_diagram-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/tum_tb_perception/ros_component_diagram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Software pipeline sketch" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The ROS-based implementation of the pose estimation solution </div> <p>The pose estimator is implemented in Python within the ROS framework as a set of modular, extensible components that are depicted above. It is available in both ROS 1 (<em>noetic</em>) and ROS 2 (<em>humble</em>).</p> <hr> <h2 id="talk-at-the-european-robotics-forum-2025">Talk at the European Robotics Forum 2025</h2> <div class="row justify-content-center"> <div class="col-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/projects/tum_tb_perception/erf_talk_photo_eurobin-480.webp 480w,/assets/img/projects/tum_tb_perception/erf_talk_photo_eurobin-800.webp 800w,/assets/img/projects/tum_tb_perception/erf_talk_photo_eurobin-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/projects/tum_tb_perception/erf_talk_photo_eurobin.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ERF 2025 paper Talk" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Presentation at the European Robotics Forum 2025 </div> <p>This work was published in the <a href="https://link.springer.com/series/15556" rel="external nofollow noopener" target="_blank">Springer Proceedings in Advanced Robotics ((SPAR,volume 36))</a> and presented at the European Robotics Forum. I had the pleasure of being invited to give a talk and was nominated for a best paper award within the area of “AI for robotics”.</p> <hr> <h2 id="conclusions-and-future-work">Conclusions and Future Work</h2> <p>This project produced a reusable, robot perception skill in the spirit of the euROBIN project’s objectives of the collaborative development of transferable robot software. Using the basic principles of transfer learning and direct depth data processing, this solution enables data-efficient object recognition and 3D position estimation, which are easily adaptable to any object(s) for similar manipulation tasks. The software package was designed to be easy to integrate in other perception stacks and to adapt and extend it where necessary. Its utility was demonstrated during euROBIN competitions in which multiple teams made use of its functionalities within their own frameworks.</p> <p>In the future, I hope to develop this further into a general-purpose robot vision skill, starting with a more general, task-agnostic strategy for orientation estimation.</p> <p><br></p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ERF 2025</abbr> <figure> <picture> <img src="/assets/img/publication_preview/abdelrahman_task_oriented_pose_estimation_2025.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="abdelrahman_task_oriented_pose_estimation_2025.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="abdelrahman2025poseestimation" class="col-sm-8"> <div class="title">Task-Oriented Visual Object Pose Estimation for Robot Manipulation: A Modular Approach</div> <div class="author"> <em>Ahmed Abdelrahman</em>, Peter So, Hoan Quang Le, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Abdalla Swikir, Sami Haddadin' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In European Robotics Forum 2025</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-89471-8_37" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://github.com/eurobin-wp1/tum-tb-perception" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/abdelrahman_task_oriented_pose_estimation_slides_2025.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>This paper presents a general method for object pose estimation from RGB-D camera data for robot manipulation tasks. We fine-tune off-the-shelf image detection models to recognize certain objects in color images then combine the result with point cloud information to estimate 3D object positions in a task-agnostic approach. By utilizing prior information about our manipulation task, we further estimate object orientations using additional heuristics. We demonstrate our approach and evaluate its performance on an electronic task board and release our adaptable and easy-to-integrate implementation as a re-usable software module under https://github.com/eurobin-wp1/tum-tb-perception.</p> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Ahmed Abdelrahman. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>